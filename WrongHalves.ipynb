{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Blackjack AI - WrongHalves Card Counting Strategy",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### A reinforcement learning approach to beating one of the most popular casino card games of all time.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "In order to be able to create an agent that can effectively play the game, we must first create its environment.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Dependencies\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "source": "The game includes 6 decks, cut and shuffled. It also contains the logic for the game actions hit, stand, double, and split. There is a 6:5 payout on 21.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class Blackjack:\n    def __init__(self):\n        self.player = []\n        self.dealer = []\n        self.deck = []\n        self.count = 0\n        self.player_blackjack = False\n        self.dealer_blackjack = False\n\n    def shuffle_deck(self):\n        suits = [\"Hearts\", \"Clubs\", \"Spades\", \"Diamonds\"]\n        ranks = [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"Jack\", \"Queen\", \"King\", \"Ace\"]\n        self.deck = [(rank, suit) for suit in suits for rank in ranks] * 6\n        random.shuffle(self.deck)\n\n        # Simulate cutting the deck by splitting it at a random point\n        cut_point = random.randint(60, 100)  # Adjust the range as desired\n        self.deck = self.deck[cut_point:] + self.deck[:cut_point]\n\n\n    def deal_card(self, hand):\n        if len(self.deck) < 52:  # Reshuffle when one deck's worth of cards is left\n            self.shuffle_deck()\n        card = self.deck.pop()\n        hand.append(card)\n        self.update_count(card)\n        return card\n\n    def update_count(self, card):\n        rank, _ = card\n        #this is wrong halves * 2\n        if rank in [\"3\", \"4\", \"6\"]:\n            self.count += 2\n        elif rank in [\"5\"]:\n            self.count += 3\n        elif rank in [\"7\",\"2\"]:\n            self.count += 1\n        elif rank in [\"8\"]:\n            self.count += 0\n        elif rank in [\"Ace\",\"10\", \"Jack\", \"Queen\", \"King\"]:\n            self.count -= 2\n\n    def hand_value(self, hand):\n        value = 0\n        aces = 0\n        for rank, _ in hand:\n            if rank in [\"Jack\", \"Queen\", \"King\"]:\n                value += 10\n            elif rank == \"Ace\":\n                aces += 1\n                value += 11\n            else:\n                value += int(rank)\n        while value > 21 and aces:\n            value -= 10\n            aces -= 1\n        return value\n\n    def can_double(self):\n        return len(self.player) == 2\n\n    def can_split(self):\n        return len(self.player) == 2 and self.player[0][0] == self.player[1][0]\n\n    def start_game(self):\n        self.shuffle_deck()\n        self.player = [self.deal_card(self.player), self.deal_card(self.player)]\n        self.dealer = [self.deal_card(self.dealer), self.deal_card(self.dealer)]\n        self.player_blackjack = self.hand_value(self.player) == 21\n        self.dealer_blackjack = self.hand_value(self.dealer) == 21\n\n    def get_state(self):\n        return {\n            \"player_value\": self.hand_value(self.player),\n            \"dealer_value\": self.hand_value([self.dealer[0]]),\n            \"player_blackjack\": self.player_blackjack,\n            \"dealer_blackjack\": self.dealer_blackjack,\n            \"can_double\": self.can_double(),\n            \"can_split\": self.can_split(),\n            \"count\": self.count\n        }\n\n    def step(self, action, bet):\n        if action == \"hit\":\n            self.deal_card(self.player)\n        elif action == \"stand\":\n            pass\n        elif action == \"double\":\n            bet *= 2\n            self.deal_card(self.player)\n        elif action == \"split\":\n            # Simplified splitting: only one additional card per split hand\n            self.player = [self.player[0], self.deal_card([])]\n            self.dealer = [self.dealer[0], self.deal_card(self.dealer)]\n        while self.hand_value(self.dealer) < 17:\n            self.deal_card(self.dealer)\n        reward = self.calculate_reward(bet)\n        next_state = self.get_state()\n        done = True\n        return next_state, reward, done\n\n    def calculate_reward(self, bet):\n        player_score = self.hand_value(self.player)\n        dealer_score = self.hand_value(self.dealer)\n        if self.player_blackjack and self.dealer_blackjack:\n            return 0\n        elif self.player_blackjack:\n            return int(1.2 * bet)  # Payout for Blackjack is now 6:5\n        elif self.dealer_blackjack:\n            return -bet\n        elif player_score > 21:\n            return -bet\n        elif dealer_score > 21 or player_score > dealer_score:\n            return bet\n        elif player_score < dealer_score:\n            return -bet\n        else:\n            return 0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": "Now we create the Q-learning agent that will effectively play our game using the WrongHalves card counting method.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class QLearningAgent:\n    def __init__(self, state_size, action_size, learning_rate=0.1, discount_rate=0.95, exploration_rate=1.0, exploration_decay=0.999, min_exploration_rate=0.01):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.learning_rate = learning_rate\n        self.discount_rate = discount_rate\n        self.exploration_rate = exploration_rate\n        self.exploration_decay = exploration_decay\n        self.min_exploration_rate = min_exploration_rate\n        self.q_table = np.zeros((state_size, action_size))\n\n    def get_action(self, state):\n        if np.random.rand() < self.exploration_rate:\n            return np.random.choice([\"hit\", \"stand\", \"double\", \"split\"]), np.random.choice([100, 200, 300])\n        action_values = self.q_table[state]\n        max_action = np.argmax(action_values[:4])  # Consider hit, stand, double, and split actions\n        max_bet = (np.argmax(action_values[4:]) + 1) * 100  # Consider bet sizes 100, 200, 300\n        return [\"hit\", \"stand\", \"double\", \"split\"][max_action], max_bet\n\n    def update(self, state, action, bet, reward, next_state, done):\n        max_next_value = np.max(self.q_table[next_state])\n        action_index = [\"hit\", \"stand\", \"double\", \"split\"].index(action)\n        #Change bet to type with MIT optimal betting according to math\n        bet_index = bet // 100 + 3  # Convert bet size to index (4, 5, 6)\n        self.q_table[state, action_index] += self.learning_rate * (reward + self.discount_rate * max_next_value * (not done) - self.q_table[state, action_index])\n        self.q_table[state, bet_index] += self.learning_rate * (reward - self.q_table[state, bet_index])  # Update bet value\n\n        if done:\n            self.exploration_rate = max(self.min_exploration_rate, self.exploration_rate * self.exploration_decay)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": "We now have our game and our player. The code is written to allow the agent to now trained on 100,000 episodes where it learns how to play the game as well as how to efficiently bet.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def state_to_index(state):\n    player_value = min(state[\"player_value\"], 21) - 1  # Cap player value at 21\n    dealer_value = min(state[\"dealer_value\"], 10) - 1  # Cap dealer value at 10\n    can_double = int(state[\"can_double\"])\n    can_split = int(state[\"can_split\"])\n    count_category = min(max((state[\"count\"] + 10) // 2, 0), 10)  # Normalize count to 0-10 range and ensure it's within bounds\n    return (player_value * 10 * 2 * 2 * 11 +\n            dealer_value * 2 * 2 * 11 +\n            can_double * 2 * 11 +\n            can_split * 11 +\n            count_category)\n\ndef train_agent(episodes=100000):\n    state_size = 21 * 10 * 2 * 2 * 11  # Player value, Dealer value, Can double, Can split, Count\n    action_size = 7  # Hit, Stand, Double, Split, Bet 100, Bet 200, Bet 300\n    agent = QLearningAgent(state_size, action_size)\n\n    for episode in range(episodes):\n        game = Blackjack()\n        game.start_game()\n        state = state_to_index(game.get_state())\n\n        done = False\n        while not done:\n            action, bet = agent.get_action(state)\n            next_state, reward, done = game.step(action, bet)\n            next_state_index = state_to_index(next_state)\n            agent.update(state, action, bet, reward, next_state_index, done)\n            state = next_state_index\n\n        if (episode + 1) % 10000 == 0:\n            print(f\"Episode {episode + 1}/{episodes}, Exploration rate: {agent.exploration_rate:.4f}\")\n\n    return agent\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": "The code below allows us to evaluate our agent on 1000 rounds and determine the total monetary gain and total reward.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def evaluate_agent(agent, episodes=1000):\n    total_rewards = 0\n    for episode in range(episodes):\n        game = Blackjack()\n        game.start_game()\n        state = state_to_index(game.get_state())\n        done = False\n        while not done:\n            action, bet = agent.get_action(state)\n            next_state, reward, done = game.step(action, bet)\n            state = state_to_index(next_state)\n            total_rewards += reward\n    average_reward = total_rewards / episodes\n    return average_reward\n\ndef evaluate_agent(agent, episodes=1000):\n    total_rewards = 0\n    wins = 0\n    losses = 0\n    for episode in range(episodes):\n        game = Blackjack()\n        game.start_game()\n        state = state_to_index(game.get_state())\n        done = False\n        while not done:\n            action, bet = agent.get_action(state)\n            next_state, reward, done = game.step(action, bet)\n            state = state_to_index(next_state)\n            total_rewards += reward\n            if done:\n                if reward > 0:\n                    wins += 1\n                elif reward < 0:\n                    losses += 1\n    average_reward = total_rewards / episodes\n    return total_rewards, average_reward, wins, losses\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": "Now, finally, we can run the main method and have the training/testing.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "if __name__ == \"__main__\":\n    trained_agent = train_agent()\n    print(\"Training complete!\")\n    total_reward, average_reward, wins, losses = evaluate_agent(trained_agent)\n    print(f\"Total reward over 1000 rounds: {total_reward}\")\n    print(f\"Average reward per round: {average_reward}\")\n    print(f\"Wins: {wins}, Losses: {losses}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Episode 10000/100000, Exploration rate: 0.0100\nEpisode 20000/100000, Exploration rate: 0.0100\nEpisode 30000/100000, Exploration rate: 0.0100\nEpisode 40000/100000, Exploration rate: 0.0100\nEpisode 50000/100000, Exploration rate: 0.0100\nEpisode 60000/100000, Exploration rate: 0.0100\nEpisode 70000/100000, Exploration rate: 0.0100\nEpisode 80000/100000, Exploration rate: 0.0100\nEpisode 90000/100000, Exploration rate: 0.0100\nEpisode 100000/100000, Exploration rate: 0.0100\nTraining complete!\nTotal reward over 1000 rounds: 19940\nAverage reward per round: 19.94\nWins: 468, Losses: 461\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 14
    }
  ]
}